---
title: "Assignment2"
output:
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

# Tasks

**Note**:

• Dataset is imbalanced. • Features are categorical (Nominal, Ordinal, Binary) and numerical. • Missing imputation does not seem to be needed in your pipeline. • Use nominal and ordinal polytomous models. • Propose a hierarchical logit approach to predict right, center and left wing voting in the political spectrum.

## Data Preparation

**Univariate Descriptive Analysis (to be included for each variable) [Eliya]:**

• Original numeric variables corresponding to qualitative concepts are present then they have to be converted to factors. • Original numeric variables corresponding to real quantitative concepts are kept as numeric but additional factors should also be created as a discretization of each numeric variable. • Exploratory Data Analysis for each variable (numeric summary and graphic support).

**Data Quality Report [Achraf]:** Per variable, count: • Number of missing values • Number of errors (including inconsistencies) • Number of outliers • Rank variables according the sum of missing values (and errors). Per individuals, count: • number of missing values • number of errors, • number of outliers • Identify individuals considered as multivariant outliers.

Create variable adding the total number missing values, outliers and errors. Describe these variables, to which other variables exist higher associations. • Compute the correlation with all other variables. Rank these variables according the correlation • Compute for every group of individuals (group of age, size of town, singles, married, ...) the mean of missing/outliers/errors values. Rank the groups according the computed mean.

**Profiling [Eliya]:** • Polytomous Target: 6 parties • Polytomous Target: right/center/left orientation.

ROSE package for balancing data

## Modeling

-   Train and test split

-   (use set.seed(your birthday))

-   Model reasonable factors as numeric variables also using transformations if needed.

-   Grouping levels in factors is allowed.

-   Adding factor main effects to the best model containing numeric variables

-   Adding factor main effects and interactions (limit your statement to order 2) to the best model containing numeric variables.

-   Goodness of fit and Model Interpretation for each proposal (nominal/ordinal).

-   Goodness of fit and Model Interpretation for political orientation (right/center/left). Make your own allocation of political parties to the right/center/left wing orientation.

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()
# Clean workspace
rm(list=ls())
```

# Load Data

```{r}

#load libraries
library("nnet")

#load data
data("gles", package = "MNLpred")
df <- gles
summary(df)
```

We can see the summary and nothing too suspicious when look at the data. In addition, no nulls or blanks in the data.

# Preprocessing

## Format and erros

```{r}
sapply(df, class)
```

We have a lot of categorical variables which have a numerical type. For a better understanding it will be converted to factor taking into account metadata.

```{r}
# Transform variables to factors
#df$vote <- factor(df$vote)
#df$egoposition_immigration <- factor(df$egoposition_immigration, ordered = TRUE)
#df$ostwest <- factor(ifelse(df$ostwest==0,"No","Yes"))
#df$political_interest <- factor(df$political_interest, ordered=TRUE)
#df$income <- factor(df$income, ordered=TRUE)
#df$gender <- factor(ifelse(df$gender==0,"Male","Female"))

df$vote <- factor(df$vote)
df$egoposition_immigration <- factor(df$egoposition_immigration)
df$f.ostwest <- factor(df$ostwest, labels = c("west","east"))
df$political_interest <- factor(df$political_interest)
df$income <- factor(df$income)
df$f.gender <- factor(df$gender,labels = c("male","female"))
summary(df)

#round(prop.table(summary(df$vote)),2)
```

Now we can understand better the data and we can see that we that we have a balance in gender. Also the majority of people is from Eastern Germany.

Now let's try to find errors and inconsistencies in format

```{r}
which(df=="") #no blanks found in the data

sapply(df, unique) # Check unique values for each attribute

df$error = 0
number_of_errors_per_attribute = c(rep(0, 6));number_of_errors_per_attribute

summary(df)

```

Taking into account metadata, no errors where found in the data since all values are in the specification.

## Outliers

Since data is all categorical there is no way to detect outliers. However, we will label the lest frequent categories as outliers.

### Univariate

Let's see the frequency of each level in each attribute

```{r}


number_of_outliers_per_attribute = c()

df$outlier = 0

# vote
prop.table(table(df$vote))
df[which(df$vote == "AfD"),"outlier"] <- 1
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute, length(which(df$vote == "AfD")))

 # egoposition_immigration
prop.table(table(df$egoposition_immigration))
df[which(df$egoposition_immigration == 8),"outlier"] <- 1
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute, length(which(df$egoposition_immigration == 8)))

#ostwest
prop.table(table(df$f.ostwest))
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute, 0)

# political_interest
prop.table(table(df$political_interest))
df[which(df$political_interest == 0),"outlier"] <- 1
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute, length(which(df$political_interest == 0)))

# income
prop.table(table(df$income))
df[which(df$income == 0),"outlier"] <- 1
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute, length(which(df$income == 0)))

# gender
prop.table(table(df$f.gender))
#df[which(df$political_interest == 0),"outlier"] <- 1
number_of_outliers_per_attribute <- append(number_of_outliers_per_attribute,0)


str(number_of_outliers_per_attribute)
```

The levels that are "outliers" won't be removed. However, it's important to keep in mind those classes that are a minority in case in further iterations levels are grouped.

### Multivariate

Detecting multivariate outliers it's not an easy process since all data is factors and we can't apply methods like mahalanobis or cooks distance. One solution could be using MCA (factorial method) to detect anomalies.

## Missing data

There is no missing data so no imputation is needed.

```{r}
is.null(df) #no nulls in the data
number_of_na_per_attribute = c(rep(0, 6));number_of_na_per_attribute
df$na = 0
```


The results are not great

## Data quality report

Let's create the report for each attribute and per individual takining into account the results obtained in the different processes.

```{r}
# per variabe
attributes_report  <- data.frame(attributes = c(names(df[,c(1:6)])), errors=number_of_errors_per_attribute, outliers=number_of_outliers_per_attribute, na=number_of_na_per_attribute)

print(attributes_report)

# per individual
individuals_report = df[,c("error","outlier", "na")]
print(individuals_report)

```

Let's compute the most problematic atrributes

```{r}

attributes_report$total <- attributes_report$error +  attributes_report$outlier + attributes_report$na
attributes_report = attributes_report[order(attributes_report$total,decreasing = TRUE),]
print(attributes_report)
```

Finally let's check the most problematic individuals

```{r}

individuals_report$total = individuals_report$error + individuals_report$outlier + individuals_report$na
individuals_report = individuals_report[order(individuals_report$total,decreasing = TRUE),]

print(individuals_report)
```

As we can see in general, data quality is pretty good. There are no erros, outliers or missing data. The only thing it might need is to group some levels in further iterations. We leveled that categories that are really low in comparison to the others as outliers so we can have in mind which are the least frequent and be able to rank them (they are not really outliers).

```{r}
summary(df)
df <- df[,c(1:8)] # Remove attributes that are used for the report

```

# Univariate Descriptive Analysis

```{r}
summary(df)
```

First, we turn qualitative,nominal and ordinal variables into factors. When it's east/west or gender we trun it into a factor with the actual name.

We can see the summary and nothing too suspicious when look at the data. We can see CDU/CSU party gets most votes on our sample, majority of voters are open for immigration (around 3,4,5), most voters are from the east of Germany, most voters have a plus average intrest in the elections, most voters income is relativity high because we can see that the variable (before being a factor) has a mean and median around 3 and as a factor we see 582 with income 3. In addition, no nulls or blanks in the data. From the boxplot we can see that income and east/west have points outside the box but we know why is it. When ploting the data we don't see anything suspicious. When define political orientation we see most voters are center, then left and the less is right, we can see that 2/3 of the voters are voting for the center parties.

```{r}
library(car)
boxplot(df[,c(1:2,4:5,7:8)])
plot(df[,c(1:2,4:5,7:8)])

round(prop.table(summary(df$vote)),2)

df$f.vote <- ''
df[which(df$vote=="AfD"),9] <- "right"
df[which(df$vote=="CDU/CSU"),9] <- "center"
df[which(df$vote=="FDP"),9] <- "center"
df[which(df$vote=="Gruene"),9] <- "left"
df[which(df$vote=="LINKE"),9] <- "left"
df[which(df$vote=="SPD"),9] <- "center"

df$f.vote <-factor(df$f.vote)
summary(df)
round(prop.table(summary(df$f.vote)),2)
library(GGally)
ggpairs(df[,c(1:2,4:5,7:8,9)])
```

```{r}
library(FactoMineR)
res.cat_f.vote <- catdes(df[,c(2,4,5,7:9)],num.var=6);res.cat_f.vote

res.cat_vote <- catdes(df[,c(1,2,4,5,7,8)],num.var=1);res.cat_vote

```

Profile and Feature Selection - we are going to look at two target variables, vote and f.vote *f.vote:* When we analysis the categorical variables we see voters for center parties are most significant when immigration policy is 5 so quite in the middle between open to not. Most of them are from the east part while voters from the west part are least significant. We can also see the income=4 is significant so more stable financially voters are for the center parties. Left parties voters are characterized with open views on immigration, both male and female and high income.

Right parties are mostly against immigration, they don't have much interest in politics, mostly male voters and quite low income voters. *vote* We can see that for almost all parties the most significant value is immigration views. There is only one right party in the data so the inferring is like the above. CDU/CSU voters immigration openness have a wide spectrum (lowest is 1 and highest is 7). income = 2 so quite in the middle at that aspect. FDP is most significant for immigration while it can be 6, 0 or 2. Gruene is most significant for immigration 2, mostly female voters with quite high political interest. LINKE voters significantly when they from the west part of Germany and open for immigration policy. SPD voters are significant to have openness for immigration and low political interest.

# Modeling

Once data is preprocessed and understanded now we will start modeling and experimenting with the different logistic models. This process will be iterative and we will try mainly the nominal, ordinal and herarchical propousal

The naming we used is mnX for nominal models, moX for ordinal models and mhX for herarchical models.

## Grouping levels

In order to try to ther different models we will grouping of levels.

```{r}

df$f.pos_imm <- factor(ifelse(df$egoposition_immigration %in% c(0,1,2,3),"open",ifelse(df$egoposition_immigration %in% c(4,5,6,7),"mild","restrictive")))

df$fo.vote <- factor(df$vote,levels = c("AfD","CDU/CSU","FDP","Gruene","LINKE","SPD"),ordered = T) 

df$f.vote_center<-factor(ifelse(df$f.vote=="center", "center","other"))
summary(df)
```

We created a variable **pos_imm** that groups the 10 levels of the variable egoposition_immigration in 3 variables: open, mild, restrictive.

## Nominal proposal

### Train test split

Splitting for train and test before starting any modeling

```{r}
set.seed(1310)

t <- sample(1:nrow(df),round(0.66*nrow(df),0)) # rows for training
train <- df[t,]  # working/training set
test <- df[-t,]  # testing set
col(train)
summary(train)
```

### Baseline model

We are starting with the null model and then looking at a model with numerical variables, since we only have numerical qualitative (and not numerical quantitative variable) we are going to start with them. Then, we run step() to get the best model so far using the AIC (we get deviance of 906.63 and AIC 950.63). We run anova() and we see we cannot reject the null hypothesis and the models are equivalent so it is better for us the small model when we already know it's significant that they are equivalent

```{r}
library(nnet)
mn0 <- multinom(vote ~ 1, data = train) #we start from the null model
summary(mn0)
mn1 <- multinom(vote ~ egoposition_immigration+political_interest+income, data = train)
summary(mn1)
#perform step for the best model so far
mn2 <- step(mn1)
summary(mn2)
anova(mn1,mn2)#we cannot reject the null hypothesis so it's statistically significant to say models are equivalent
AIC(mn1,mn2)
```

### Improving the model

#### Adding factor and interactions

Once we have the best model at this point we are adding factors to the model whether it's as interactions or additive to the model. Since we don't have order between left center right we use multinom() and not polr(). We run step() to get the best model with the factors. We have got better AIC for m4

```{r}
mn3 <- multinom(vote ~ egoposition_immigration*f.ostwest + egoposition_immigration*f.gender + egoposition_immigration+f.gender + f.ostwest + f.gender*f.ostwest, data = train) #adding factors to the model
summary(mn3)
mn4 <- step(mn3)
summary(mn4)
AIC(mn4,mn2)
```

#### Try: Model with grouped target

Model with target variable f.vote only

ow, we are taking f.vote which has 3 levels, each party is classified to 3 orientations: left, center or right. When we run the null model we already see a better AIC up to this point. Then we are doing the same process as above where we building a model with numerical qualitative variables, finding the best model so far, adding factors to the model and finding the best model so far with the factors. When we have nested models we are checking anova() to do hypothesis test to whether or not the models are equivalent. lastley, we check AIC for all models to see which one has the lowest. mm4 has Deviance = 890.39 AIC = 942.39. When we compare m4 and mm4 we get a better AIC for mm4 and that is when the parties are classified to 3 orientations.

```{r}

mn5 <- multinom(f.vote ~ 1, data = train) #we start from the null model
summary(mn5)
#we do not have numerical quantitative variable and since all of the rest are based on numerical nominal variables we will use all of them
mn6 <- multinom(f.vote ~ egoposition_immigration+political_interest+income, data = train)
summary(mn6)
#perform step for the best model so far
mn7 <- step(mn6)
summary(mn7) #best model so far has D() of 906.63 and AIC 950.63
anova(mn6,mn7) #we cannot reject the null hypothesis and the models are equivalent so it is better for us the small model when we already know it's significant that they are equivalent
mn8 <- multinom(f.vote ~ egoposition_immigration*f.ostwest + egoposition_immigration+f.ostwest + egoposition_immigration*f.gender + egoposition_immigration+f.gender + f.gender*f.ostwest, data = train) #adding interactions and additive of the factors
summary(mn8)
mn9 <- step(mn8)
summary(mn9) #best model so far we got f.vote ~ egoposition_immigration + f.ostwest + f.gender
#results are Deviance = 886.0369 AIC = 942.0369
#since we don't have order between left center right we use multinom() and not polr()
anova(mn8,mn9)
AIC(mn5,mn6,mn7,mn8, mn9)
AIC(mn4,mn9)
```

#### Try: Model with grouped immigration

Adding 3 levels of immigration policies to see if it will improve the current results we have.

```{r}
summary(mn4)

mn10 <- multinom(vote ~ f.pos_imm+ f.ostwest +  f.gender, data = train)
summary(mn10)
mn11 <- step(mn10) #the null model is the best in such case
summary(mn11) 

# factors and iunteractions
mn12 <- multinom(vote ~ f.pos_imm*f.gender + f.pos_imm*f.ostwest + f.gender*f.ostwest + f.pos_imm + f.gender + f.ostwest, data = train)
summary(mn12)
mn13 <- step(mn12)
summary(mn13)
anova(mn12,mn13)


mn14 <- multinom(vote ~ f.pos_imm*f.ostwest + f.pos_imm+f.ostwest + f.pos_imm*f.gender + f.pos_imm+f.gender, data = train)
summary(mn14)
mn15 <- step(mn14)
summary(mn15)
AIC(mn4,mn15)

# comparison of all models
AIC(mn1, mn2, mn3, mn4, mn10,mn11,mn12,mn13, mn14, mn15) 
####maybe we dont have to present them all, because the model after the step necessarily has lower AIC####
summary(mn10)
summary(mn15)
```

Aggregating the levels of egoposition_immigrations improve a little the results of the modeling part.

The model chosen as the best model is the model mn10 which formula is the following: formula = vote \~ f.pos_imm + f.ostwest + f.gender

This model does not have the lowest Akaike but it's not much different from the best model (mn15) and it's more simple and it would be easy to explain.

### Goodness of fit and model interpretation

**Goodness of fit**

```{r}

mn10$deviance;mn10$edf;2*nrow(df)-mn10$edf
1-pchisq(mn10$deviance, 2*nrow(df)-mn10$edf)
```

For the best model obtained which include f.pos_imm, f.ostwest and f.gender has a p value of 0.1987459. This means that we fail to reject H0 where the model is consistent to data so we conclude by saying model fit's well the data.

**Model interpretation**

We will interpret the different we will interpret the effects of the different variables into of f.ostweseast and f.genderfaemale.

```{r}
coef(mn10)

#f.pos_immopen
coef(mn10)[,2]
exp(coef(mn10)[,2])
100*(exp(coef(mn10)[2])-1)

#f.pos_immrestrictive
coef(mn10)[,3]
exp(coef(mn10)[,3])
100*(exp(coef(mn10)[3])-1)

#f.ostwesteast
coef(mn10)[,4]
exp(coef(mn10)[,4])
100*(exp(coef(mn10)[,4])-1)

#f.genderfemale
coef(mn10)[5]
exp(coef(mn10)[,5])
100*(exp(coef(mn10)[,5])-1)
```

With open position in immigration we can see that they have an increasce in the logodds of 2.32 for Gruene, 2.19 for LINKE, 2.007 for SPD, 0.871 FOR CDU/CSU and 0.509 for ADP. The increase is similar for the tree left partys. This makes sense since voters that are from left are more likely to be open about immigration. At the same time we can see that the one with lowest score is ADP that is a party from rights.

People that have a restrictive postition in immigration we get a decreasce in the logodds for all partys. Especially Gruene (-16.06) which is a left party. The other parties have a similar decreasce -2.003 FOR CDU/CSU, -0.079 for FDP, -1.902 for LINKE, -1.84 for SPD.

The log odds of being in from the east of germany will increase by 0.879 fior CDU/CSU, 0.88 for FDP, 0.896 for Gruene, 0.09 for LINKE, 0.74fSPD. The increasce is similar for three of the candidates and then we can see that SPD is following by not that far. Finally LINKE seems the least increasced by people from the east.

Being a female we can see different outcomes where there is also an increase of 1.22 for CDU/CSU, 1.32 for FDP, 1.78for Gruene, 1.24 for LINKE and 1.38 for SPD. In this case, all have a similar incease. However, Gruene is the one being the most benefited of that level.

**Prediction of probabilities**

```{r}
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("mild"), f.ostwest = factor("west"), f.gender=factor("male")))
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("mild"), f.ostwest = factor("west"), f.gender=factor("female")))
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("mild"), f.ostwest = factor("east"), f.gender=factor("female")))
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("mild"), f.ostwest = factor("east"), f.gender=factor("male")))
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("open"), f.ostwest = factor("east"), f.gender=factor("male")))
predict(mn10, type="probs", newdata=data.frame(f.pos_imm=factor("restrictive"), f.ostwest = factor("east"), f.gender=factor("male")))
```

From this results we obtained that males and females from east and west with a mild pos_imm are more likely to vote for CDU/CSU and then SPD. So this means that gender and being from the west are not big factors in the predictibility of the model.

When predicting using pos_imm we can see that the probabilities actually change a lot. From open we can see that the most probably partys are SPD, GRUENE and CDU/CSU(left-center). And when restrictrive it changes to AfD and CDU/CSU(right-center). So this model depending on if the immigration factor is open or restrictive it will change to be more likekly to vote to the left or right partys.

**Effects**

```{r}
library(effects)
plot(Effect(focal.predictors = c("f.pos_imm"), mn10))
plot(Effect(focal.predictors = c("f.gender"), mn10))
plot(Effect(focal.predictors = c("f.ostwest"), mn10))
```

From the effects plot we can see that AfD is getting a big probability when pos_imm is restrictive and we can see that SPD and Gruene and LINKE are getting better probabilty when pos_imm is open. From f.gender and f.ost there are no big insights to gather.

**Predictive power**

Let's check the predictive power of the model using both train and test data and find the performance metrics of the model.

On train

```{r}

tt<-table(predict(mn10),train$vote);tt #Checks that the model i not predicting part times

tt
100*sum(diag(tt))/sum(tt) # ACCURACY of the model
```

On test

```{r}
tt<-table(predict(mn10, newdata = test),test$vote);tt #Checks that the model i not predicting part times
100*sum(diag(tt))/sum(tt) # ACCURACY of the model


predicted <- predict(mn10, newdata = test)
actual <- test$vote

library(caret)

confusionMatrix(predicted, actual, mode = "everything")
```

We can see that the best model found get's a 37% of accuracy on train and  32% of accuracy on test. Those are not great results. Maybe there are more important explanatory variables that are missing in the data.

## Ordinal proposal

The target variable does not have a natural order to get so it's better to use nominal models. However we prupose an order to try also ordinal models. The order we prupose is the following 

- AfD\<CDU/CSU\<FDP\<Gruene\<LINKE\<SPD

### Baseline model

```{r}
library(MASS)
mo0 <- polr(fo.vote ~ 1 , data = train)
summary(mo0)
summary(train)
mo1 <- polr(fo.vote ~ political_interest+income, data = train) #numeric qualitative variables
summary(mo1)
mo2 <- step(mo1)
summary(mo2)

```


### Improving the model

#### Adding factors and interactions

```{r}
mo3 <- polr(fo.vote ~ f.pos_imm*f.gender + f.pos_imm*f.ostwest + political_interest*f.ostwest + f.gender*political_interest + f.pos_imm + political_interest + f.ostwest + f.gender + f.pos_imm*political_interest + f.gender*f.ostwest , data = train,Hess = TRUE) #adding factors #WE GET A WARNING MESSAGE HERE WHICH I THINK IT'S ODD, IT DOESNT LET ME GENERATE SUMMARY UNLESS I ADD Hess = TRUE
###### Missings: f.pos_imm*political_interest + f.gender*f.ostwest #######
summary(mo3)
mo4 <- step(mo3)
summary(mo4)
anova(mo4,mo3)
AIC(mo4,mo3)

# mo4 Akaike 2131
```

#### Try: Grouped target

```{r}
mo5 <- polr(f.vote ~ political_interest+income, data = train) #numerical qualitative variables
summary(mo5)
mo6 <- step(mo5)
summary(mo6)

mo7 <- polr(f.vote ~ f.pos_imm*f.gender + f.pos_imm*f.ostwest + political_interest*f.ostwest + f.gender*political_interest + f.pos_imm + political_interest + f.ostwest + f.gender + f.pos_imm*political_interest + f.gender*f.ostwest , data = train,Hess = TRUE) #adding factors
summary(mo7)
mo8 <- step(mo7)
summary(mo8)
mo8$zeta
clogodd1 <- mo8$zeta[1];clogodd1 #values of linear predictor
clogodd2 <- mo8$zeta[2];clogodd2
gam1 <-exp(clogodd1)/(1+exp(clogodd1));gam1
gam2 <- exp(clogodd2)/(1+exp(clogodd2));gam2
pCenter <- gam1;pCenter
pLeft <- gam2-gam1;pLeft
pRight <- 1-gam2;pRight
pCenter+pLeft+pRight==1 #return true
AIC(mo4,mo8) #we get a better AIC when we aggregate the different parties into 3 political ideologies
```


The model with grouped variable gave us a lower Aikaike score but since it's aggregated target, we decided to keep the target with the 6 ordered levels.

The model chosen as the best model is the model mo4 which formula is the following: formula = fo.vote ~ f.pos_imm + f.gender +  f.ostwest + political_interest + f.gender:f.ostwest.

### Goodness of fit and model interpretation


**Goodness of fit**

```{r}

mo4$deviance;mo4$edf;2*nrow(df)-mo4$edf
1-pchisq(mo4$deviance, 2*nrow(df)-mo4$edf)
```

For the best model obtained which include egoposition_immigration, f.gender and political_interest has a p value of 0.03143539 This means that we reject H0 where the model is consistent to data so we conclude by saying model does not fit well data.

**Model interpretation**

We will interpret the different we will interpret the values of the coefficients obtained.

```{r}

summary(mo4)
coef(mo4)
mo4$zeta

summary_table <- coef(summary(mo4))
pval <- pnorm(abs(summary_table[, "t value"]),lower.tail = FALSE)* 2
summary_table <- cbind(summary_table, "p value" = round(pval,3))
summary_table
```

As we can see the we get a coefieicient of 0.945 for the open level and  -1.104 for the restrictive level of pos_imm. Female h as a coefficient of -0.06 and finally, for each political_interest variable there are coefficient values around 3. Also there is the interaction of being a female and being from the west which has a coefficient of 0.53377. Finally, if we check the p values, the variable of f.pos_imm and political_interest have a p value less than 0.5 which means that they are statistically significant at a 95 CI.


Also from the intercepts, we can see the cumulative logodds and we can see that voting for AfD vs the other parties is 0.668 and voting for AfD or CDU/CSU it goest to 3.000 vs other parties. Then it's more stable and it does not change that much so CSU/CSU takes into account a lot of the cumulative logodss.


**Prediction of probabilities**

```{r}

predict(mo4, type="probs", newdata=data.frame(f.pos_imm=factor("mild"),  f.gender=factor("male"), f.ostwest = factor("east"),political_interest=factor(2)))
predict(mo4, type="probs", newdata=data.frame(f.pos_imm=factor("open"), f.gender=factor("male"),f.ostwest = factor("west"),political_interest=factor(4)))
predict(mo4, type="probs", newdata=data.frame(f.pos_imm=factor("restrictive"), f.gender=factor("male"),f.ostwest = factor("west"),political_interest=factor(2)))
predict(mo4, type="probs", newdata=data.frame(f.pos_imm=factor("restrictive"), f.gender=factor("male"),f.ostwest = factor("west"),political_interest=factor(0)))
```

As we can see in the model follows a similar behavior as the nominal model: restrictive about immigration and less political interest are more likely to vote for rights and on the other hand, open and high political_interest are more likely to vote for left and so on. The other variables (gender and f.ostwest) don't have to seem to much impact in predictive outcome.

**Effects**

```{r}
library(effects)
plot(Effect(focal.predictors = c("f.pos_imm"), mo4))
plot(Effect(focal.predictors = c("f.ostwest"), mo4))
plot(Effect(focal.predictors = c("f.gender"), mo4))
plot(Effect(focal.predictors = c("political_interest"), mo4))
```

For this model we can see that the effects are similar as the nominal model. The main inights we can gather are that restrictive people in immigration are also more likely to votge for CDU/CSU (right-center). Also for political interest, people with 0 political interest have a 60% of probability to vote for AfD (right).


**Predictive power**

Let's check the predictive power of the model using both train and test data and find the performance metrics of the model.

On train

```{r}

tt<-table(predict(mo4),train$fo.vote);tt #Checks that the model i not predicting part times

tt
100*sum(diag(tt))/sum(tt) # ACCURACY of the model
```

On test

```{r}
tt<-table(predict(mo4, newdata = test),test$fo.vote);tt #Checks that the model i not predicting part times
100*sum(diag(tt))/sum(tt) # ACCURACY of the model


predicted <- predict(mo4, newdata = test)
actual <- test$fo.vote

library(caret)

confusionMatrix(predicted, actual, mode = "everything")
```

We can see that the best model found get's a 35% of accuracy on train and  31% of accuracy on test. Those are not better results than the nominal proposal.

## Herarchical proposal

The final propousal is the herarchical which will require to group target variable into 3 levels: left, center, right. 
We have 665 people hwo are laveled as "center", 266 as "left" and 69 as "right". This is a problem since target is clearly unbalanced.

We will try an herarchical model with people who votes for the center (majority) and people who vote for other parties (left or right).

-- 1st  CENTER OTHER
-- 2nd       LEFT RIGHT

### Baseline model

```{r}
#we use here the same train and test as above.

mh0 <- glm(f.vote_center ~ 1, family = "binomial", data = train)
summary(mh0)
mh1 <- glm(f.vote_center ~ political_interest+income, family = "binomial", data = train)
summary(mh1)
mh2 <- step(mh1)
summary(mh2)

```

Grouping variables 2 levels improved drastically the aikaike score.

### Improving the model

#### Adding factors and interactions

```{r}
# First level
mh3 <- glm(f.vote_center ~ f.pos_imm*f.ostwest + f.pos_imm+f.ostwest + f.pos_imm*f.gender + f.ostwest*f.gender + f.pos_imm+f.gender, family = "binomial", data = train)

summary(mh3)
mh4 <- step(mh3)
summary(mh4)
#comparision
anova(mh3, mh4)
AIC(mh0, mh1,mh2,mh3,mh4)

## --- Second level
mh5 <- glm(f.vote ~ f.pos_imm+political_interest+income, family = "binomial", data = train[train$f.vote_center=="other",])
summary(mh5)
mh6 <- step(mh5)
summary(mh6)
mh7 <- glm(f.vote ~ f.pos_imm*f.ostwest + f.pos_imm*f.gender + political_interest*f.ostwest + political_interest*f.gender + f.pos_imm + political_interest + f.ostwest + f.gender +  political_interest*f.pos_imm + f.ostwest*f.gender, family = "binomial", data = train[train$f.vote_center == "other", ])

summary(mh7)
mh8 <- step(mh7)
summary(mh8)
AIC(mh8,mh7)

AIC(mh8)+AIC(mh4)

```

### Model interpretation


**Model interpretation**

We will interpret the different we will interpret the values of the coefficients obtained.

```{r}

summary(mh4)
coef(mh4)

summary(mh8)
coef(mh8)
```

As we can see the we get a coefieicient of 0.279 for the open level and 2.0178 for the restrictive level of pos_imm. eastern people have a -0.458 of coefficiient and females as a coefficient of -0.09 and finally, for each political_interest variable there are coefficient vvalues around 3. We can see that restrictive level pos_imm and the interaction between restrictive and being a female is statsitifically sifnificant for a 95.

**Effects**

```{r}
library(effects)
plot(allEffects(mh4))
plot(allEffects(mh8))
summary(mh8)
```

As we can see from the plot, we can see that people from the east and that they are open about immigration are more likely to vote center. At the same time people that are restrictive and are from the west have a higher probability to vote center as well. 

Also we can see that men who are restrictive and females who are open are more likely to vote for center.

However, we can see in the effects plot tthat the confidence interval is very wide.

**Predictive power**

Let's check the predictive power of the model using both train and test data and find the performance metrics of the model.


```{r}

#we can see that the HL model  gives the lowest AIC.

tt_first_level<-table( ifelse(predict(mh4,type="response")>0.5,"other","center"),train$f.vote_center);tt_first_level
tt_second_level<-table(factor(ifelse(predict(mh8,type="response")<0.5,"left","right"),levels=c("left","right")),train$f.vote[train$f.vote != "center"]);tt_second_level


100*sum(diag(tt_first_level))/sum(tt_first_level) + 100*sum(diag(tt_second_level))/sum(tt_second_level)

#hardcoded resutls
100*(440+165+30) / 660


```


We can see that the best model found get's a 96% of accuracy. The best metric so far, however it's important to keep in mind that data is highly unbalanced.

# Comparision of models and conclusions

From each propousal we iterated into the different steps and we obtained the better model for each. All models are trained with the same sample and using the same methodology. Here are the best models obtained:

- Nomina proposal: The best model we found is the mn10 which is described by the folliwng formula: vote \~ f.pos_imm + f.ostwest + f.gender

- Ordinal proposal: The best model is mo4 which is described by the following formula fo.vote ~ f.pos_imm + f.gender +  f.ostwest + political_interest + f.gender:f.ostwest.

- Herachical proposal: The best model is mh4 which is described by the following formula: f.vote_center ~ f.pos_imm * f.ostwest + f.pos_imm +  f.ostwest + f.pos_imm * f.gender + f.pos_imm + f.gender

```{r}
AIC(mn10, mo4)
AIC(mh4)+AIC(mh8) # first and second level
BIC(mn10, mo4)
BIC(mh4)+BIC(mh8)
```

As we can see the best model obtained based on AIC/BIC scores is clearly the herarchical. Also if we take into account the predictive power metrics for each one, the hierarchical model wins with 96% of accuracy vs 37% (nominal) and 35% (ordinal). Also we think that herarchical modeling is a very natural approach to the kind of data we have (taking into account that each party is labeled as a left, center, right). However,  it's important to keep in mind that the hierarchical model have an aggregated target that is unbalanced so we think that it would perform even better if we manage do get balance data.

Also for nominal and ordinal models we found out that we did not manage to get really good metrics and we think that it's because it has unbalance targets and we think that maybe there are some exploratory variables that are important factors on deciding between the 6 levels.

In this project we had to iterate for each proposal to try to obtain the best models. The data quality was very good so our main focus was in the modeling part. The main difficulties are that classes are unbalanced and also deciding on whether we should group levels of factors or not. We belive that the results could be improved. However, we iterated and experimented with different steps and methods to try to explain the results.


